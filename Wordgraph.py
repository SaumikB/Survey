# -*- coding: utf-8 -*-
"""Shagata_WordGraph_v2_Hesitant.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nsV6rFekdF2LAuscDV7bmM_1lAtvCX5L
"""

from google.colab import drive
drive.mount("/content/gdrive")

import pandas as pd
import numpy as np
import matplotlib
import matplotlib.pyplot as plt
import nltk
from gensim.utils import simple_preprocess
from gensim.parsing.preprocessing import remove_stopwords, STOPWORDS
mystop = STOPWORDS.union(set(['covid','vaccine']))

df=pd.read_excel('/content/gdrive/My Drive/DLDatasets/Shagata_paper2/merged data_1220resposes.xlsx')
df=df['Reason_hesitant']
pattern1 = '|'.join(['Side effects', 'Side effect', 'Side-effects', 'Side-effect'])
pattern2= '|'.join(['New strain', 'New strains'])
df=df.str.replace(pattern1,'SideEffects',case = False)
df=df.str.replace(pattern2,'NewStrains',case = False)
df=df.str.replace('Availability','available',case = False)
df=df.str.replace('affecting','affect',case = False)

# # df2=df['Reason_not_to_take']
# # df3=df['Reason_change_unwilling']
# # df4=df['Reason_hesitant']
# # df5=df['hesitant_more_likely']
# # df6=df['hesitant_less_likely']
# # df = pd.concat([df3])

df = df.str.replace("[^A-Za-z ]", " ")
df = df.dropna()
df_raw=df

#print(df_raw.iat[1])


str_tot=''
for i in range(len(df)):
    str_in=df.iat[i]
    str_tot=str_tot+str_in+'. '

text1=str_tot
doc_complete=[text1]

from nltk.corpus import stopwords
nltk.download('wordnet')
nltk.download('stopwords')
nltk.download('omw-1.4')
from nltk.stem.wordnet import WordNetLemmatizer
import string
stop = set(stopwords.words('english'))
stop=stop.union(set(['covid','virus','vaccine','Vaccine','Vaccines','vaccination', 'think','vaccines','taking','yes']))
exclude = set(string.punctuation)
lemma = WordNetLemmatizer()
def clean(doc):
    stop_free = " ".join([i for i in doc.lower().split() if i not in stop])
    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)
    normalized = "".join(WordNetLemmatizer().lemmatize(word) for word in punc_free)
    return normalized

doc_clean = [clean(doc).split() for doc in doc_complete]

df=pd.Series(doc_clean[0])
df=df.str.replace('sideeffects','side-effects',case = False)
df=df.str.replace('newstrains','new-strains',case = False)

from collections import Counter
results = Counter()
df.str.lower().str.split().apply(results.update)

total_term=len(results)
results_top10=results.most_common(15)
print(results_top10)

import matplotlib.pyplot as plt
from google.colab import files

matplotlib.rcParams['lines.linewidth'] = 4
plt.figure(figsize=(10, 7))
plt.title("Willing-->Hesitant",fontsize=18)
plt.xlabel('Terms',fontsize=16)
plt.ylabel('Normalized Frequency',fontsize=18)
plt.xticks(rotation='vertical',fontsize=12)
plt.yticks(fontsize=14)

names = list(dict(results_top10).keys())
values = list(dict(results_top10).values())

plt.bar(range(len(results_top10)), np.array(values)/total_term, tick_label=names,color=(0.2, 0.4, 0.6, 0.6))

plt.savefig("Why_H.png", bbox_inches='tight')
files.download("Why_H.png")
plt.show()
################## Delete 'also' 'like' 'getting' 'vaccinated' manually from the saved fig

!pip install spacy &> /dev/null
!python3 -m spacy download en &> /dev/null

A=df_raw
stop=stop.union(set(['covid','virus','vaccine','Vaccine','Vaccines','vaccination', 'think','vaccines','taking','yes']))
A = A.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))
def format_sentence(sentence) :
    sen=[]
    sentenceSplit = filter(None, sentence.split("."))
    for s in sentenceSplit :
       s=s.replace(",","")
       sen.append(s+ ".")
    return sen


import spacy
nlp = spacy.load("en_core_web_sm")

sen_tex=[]
sen_tag=[]
sent=[]
for j in range(len(A)):
  ab=A.iat[j]
  sen= format_sentence(ab)
  for i in range(len(sen)):
    y=sen[i]
    xx=[]
    tt=[]
    doc = nlp(y)
    for token in doc:
  #print(f'{token.text:{8}} {token.pos_:{6}} {token.tag_:{6}} {token.dep_:{6}} {spacy.explain(token.pos_):{20}} {spacy.explain(token.tag_)}')
      x=str(f'{spacy.explain(token.pos_)}')
      t=str(f'{token.text}')
      xx.append(x)
      tt.append(t)
    sent.append(y)
    sen_tex.append(tt)
    sen_tag.append(xx)

  sent.append('Break')
  sen_tex.append('Break')
  sen_tag.append('Break')

!pip install python-igraph==0.8.3 &> /dev/null
!apt install libcairo2-dev pkg-config python3-dev &> /dev/null
!pip install python-igraph leidenalg pycairo &> /dev/null
from igraph import *

g = Graph()
count=0
base_node=0
#topic1=['nanocomposites','method','dimensional','potential','fabrication','polymer']
topic=['SideEffects','efficacy','people','new-strains']
#topic1=['method','potential']
word_list=topic
word_label=[0,1,2,3]
for ii in range(len(topic)):
  a=topic[ii]
  g.add_vertices(1)
  g.vs[count]["label"]= a
  #g.vs[count]["type"] = "noun"
  g.vs[count]["type"] = "verb"
  count=count+1

for ii in range(len(topic)):
  word_list_temp=[]
  base_node=ii
  a=topic[ii]
  # g.vs[count]["label"]= a
  # g.vs[count]["type"] = "verb"

  for i in range(len(sent)):
    index=[]
    sen=sent[i]
    if a in sen:
      sen_t=sen_tex[i]
      sen_tg=sen_tag[i]
      # if 'noun' in sen_tg and 'verb' in sen_tg:
      #   index_noun = [i for i, x in enumerate(sen_tg) if x == 'noun']
      #   index_verb = [i for i, x in enumerate(sen_tg) if x == 'verb']
      #   index=index_noun+index_verb
      # else:
      #   if 'noun' in sen_tg:
      #     index = [i for i, x in enumerate(sen_tg) if x == 'noun']
      #   else:
      #     if 'verb' in sen_tg:
      #       index = [i for i, x in enumerate(sen_tg) if x == 'verb']

      if 'noun' in sen_tg:
        index = [i for i, x in enumerate(sen_tg) if x == 'noun']

        if len(index)>0:
          verb_l=[sen_t[i] for i in index]
          if type(verb_l) is list:
            for u in range(len(verb_l)):
              if verb_l[u] not in word_list:
                word_list.append(verb_l[u])
                # print(verb_l[u])
                g.add_vertices(1)
                word_label.append(g.vcount()-1)
                g.vs[count]["label"]=verb_l[u]
                g.vs[count]["type"] = "verb"
                g.add_edges([(base_node, count)])
                count=count+1
              else:
                ind=word_list.index(verb_l[u])
                #print(ind)
                g.add_edges([(base_node, word_label[ind])])

          else:
              if verb_l not in word_list:
                word_list.append(verb_l)
                # print(verb_l)
                g.add_vertices(1)
                word_label.append(g.vcount()-1)
                g.vs[count]["label"]=verb_l
                g.vs[count]["type"] = "verb"
                g.add_edges([(base_node, count)])
                count=count+1
              else:
                ind=word_list.index(verb_l)
                #print(ind)
                g.add_edges([(base_node, word_label[ind])])


                # if verb_l not in word_list_temp:
                #   word_list_temp.append(verb_l)
                #   ind=word_list.index(verb_l)
                #   if g.are_connected(base_node, word_label[ind])== False:
                #     g.add_edges([(base_node, word_label[ind])])

## Use for Verb-- Do not run otherwise

g.vs['label_color']='#BBB'
# g.vs.find(label="SideEffects")['label_color']='#0a780c'
# g.vs.find(label="efficacy")['label_color']='#0a780c'
# g.vs.find(label="people")['label_color']='#45063a'
# g.vs.find(label="dying")['label_color']='#0a780c'
# g.vs.find(label="experiences")['label_color']='#0a780c'
# g.vs.find(label="prolonged")['label_color']='#0a780c'
# g.vs.find(label="observe")['label_color']='#0a780c'
# g.vs.find(label="known")['label_color']='#0a780c'
# g.vs.find(label="dilemma")['label_color']='#0a780c'
# g.vs.find(label="studied")['label_color']='#0a780c'
# g.vs.find(label="Fear")['label_color']='#0a780c'
# g.vs.find(label="shown")['label_color']='#0a780c'

## Use for Noun-- Do not run otherwise

g.vs['label_color']='#BBB'
g.vs.find(label="SideEffects")['label_color']='#0a780c'
g.vs.find(label="fear")['label_color']='#0a780c'
g.vs.find(label="risks")['label_color']='#0a780c'
g.vs.find(label="weakness")['label_color']='#0a780c'
g.vs.find(label="scares")['label_color']='#0a780c'
g.vs.find(label="span")['label_color']='#0a780c'
g.vs.find(label="allergy")['label_color']='#0a780c'
g.vs.find(label="NewStrains")['label_color']='#0a780c'
g.vs.find(label="possibilities")['label_color']='#0a780c'


g.vs.find(label="efficacy")['label_color']='#0a780c'
g.vs.find(label="suffice")['label_color']='#0a780c'
g.vs.find(label="rate")['label_color']='#0a780c'
g.vs.find(label="lack")['label_color']='#0a780c'
g.vs.find(label="statistics")['label_color']='#0a780c'


g.vs.find(label="people")['label_color']='#45063a'
g.vs.find(label="friends")['label_color']='#45063a'
g.vs.find(label="fever")['label_color']='#45063a'
g.vs.find(label="family")['label_color']='#45063a'
g.vs.find(label="impact")['label_color']='#45063a'
g.vs.find(label="consensus")['label_color']='#45063a'

deg=g.vs[0].degree()
#layout = g.layout("kk")
layout = g.layout_auto()
deg = g.degree()
#plot(g,"/content/drive/My Drive/DLDatasets/my_graph.pdf",layout=layout,vertex_label_size=9,vertex_size=5, vertex_font=2, vertex_color='#FF0')
plot(g,"WhyHesitantNoun.pdf",layout=layout,keep_aspect_ratio=True, vertex_label_size=np.power(np.array(deg),0.3)*6,vertex_size=np.array(deg)*0.01, vertex_font=2, vertex_color='#F55',edge_color='#EEE',vertex_frame_color='#FFF',bbox=(600,600))
files.download("WhyHesitantNoun.pdf")

###########################################################     Reason Not to Take                      ###############################################################

df1=pd.read_excel('/content/gdrive/My Drive/DLDatasets/Shagata_paper2/merged data_1220resposes.xlsx')
df=df1['hesitant_more_likely']
df_opt=df1['hesitant_less_likely']
df = df.append(df_opt)


pattern1 = '|'.join(['Side effects', 'Side effect', 'Side-effects', 'Side-effect'])
pattern2= '|'.join(['New strain', 'New strains'])
df=df.str.replace(pattern1,'SideEffects',case = False)
df=df.str.replace(pattern2,'NewStrains',case = False)
df=df.str.replace('Availability','available',case = False)
df=df.str.replace('affecting','affect',case = False)
df=df.str.replace('Slots','slot',case = False)
df=df.str.replace('Slot','slot',case = False)
df=df.str.replace('Places','place',case = False)
df=df.str.replace('Governments','government',case = False)
df=df.str.replace('makes','make',case = False)
df=df.str.replace('Make','make',case = False)

# # df2=df['Reason_not_to_take']
# # df3=df['Reason_change_unwilling']
# # df4=df['Reason_hesitant']
# # df5=df['hesitant_more_likely']
# # df6=df['hesitant_less_likely']
# # df = pd.concat([df3])

df = df.str.replace("[^A-Za-z ]", " ")
df = df.dropna()
df_raw=df

#print(df_raw.iat[1])


str_tot=''
for i in range(len(df)):
    str_in=df.iat[i]
    str_tot=str_tot+str_in+'. '

text1=str_tot
doc_complete=[text1]

from nltk.corpus import stopwords
nltk.download('wordnet')
nltk.download('stopwords')
nltk.download('omw-1.4')
from nltk.stem.wordnet import WordNetLemmatizer
import string
stop = set(stopwords.words('english'))
stop=stop.union(set(['covid','virus','vaccine','Vaccine','Vaccines','vaccination', 'think','vaccines','taking','yes']))
exclude = set(string.punctuation)
lemma = WordNetLemmatizer()
def clean(doc):
    stop_free = " ".join([i for i in doc.lower().split() if i not in stop])
    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)
    normalized = "".join(WordNetLemmatizer().lemmatize(word) for word in punc_free)
    return normalized

doc_clean = [clean(doc).split() for doc in doc_complete]

df=pd.Series(doc_clean[0])
df=df.str.replace('sideeffects','side-effects',case = False)
df=df.str.replace('newstrains','new-strains',case = False)

from collections import Counter
results = Counter()
df.str.lower().str.split().apply(results.update)

total_term=len(results)
results_top10=results.most_common(15)
print(results_top10)

import matplotlib.pyplot as plt
from google.colab import files

matplotlib.rcParams['lines.linewidth'] = 4
plt.figure(figsize=(10, 7))
plt.title("Reasons No--Hesitant",fontsize=18)
plt.xlabel('Terms',fontsize=16)
plt.ylabel('Normalized Frequency',fontsize=18)
plt.xticks(rotation='vertical',fontsize=12)
plt.yticks(fontsize=14)

names = list(dict(results_top10).keys())
values = list(dict(results_top10).values())

plt.bar(range(len(results_top10)), np.array(values)/total_term, tick_label=names,color=(0.2, 0.4, 0.6, 0.6))

plt.savefig("ReasonNo--Hesitant.png",bbox_inches='tight')
files.download("ReasonNo--Hesitant.png")
plt.show()

!pip install spacy &> /dev/null
!python3 -m spacy download en &> /dev/null

A=df_raw
stop=stop.union(set(['covid','virus','vaccine','Vaccine','Vaccines','vaccination', 'think','vaccines','taking','yes','tech','mean', 'Guaranty','veins','members','immunity']))
A = A.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))
def format_sentence(sentence) :
    sen=[]
    sentenceSplit = filter(None, sentence.split("."))
    for s in sentenceSplit :
       s=s.replace(",","")
       sen.append(s+ ".")
    return sen


import spacy
nlp = spacy.load("en_core_web_sm")

sen_tex=[]
sen_tag=[]
sent=[]
for j in range(len(A)):
  ab=A.iat[j]
  sen= format_sentence(ab)
  for i in range(len(sen)):
    y=sen[i]
    xx=[]
    tt=[]
    doc = nlp(y)
    for token in doc:
  #print(f'{token.text:{8}} {token.pos_:{6}} {token.tag_:{6}} {token.dep_:{6}} {spacy.explain(token.pos_):{20}} {spacy.explain(token.tag_)}')
      x=str(f'{spacy.explain(token.pos_)}')
      t=str(f'{token.text}')
      xx.append(x)
      tt.append(t)
    sent.append(y)
    sen_tex.append(tt)
    sen_tag.append(xx)

  sent.append('Break')
  sen_tex.append('Break')
  sen_tag.append('Break')

!pip install python-igraph==0.8.3 &> /dev/null
!apt install libcairo2-dev pkg-config python3-dev &> /dev/null
!pip install python-igraph leidenalg pycairo &> /dev/null
from igraph import *

g = Graph()
count=0
base_node=0
topic=[ 'take', 'people', 'SideEffects', 'effective', 'vaccinated', 'safe']
word_label=[0,1,2,3,4,5]

# topic=['availability','available',]
# word_label=[0,1]

word_list=topic

for ii in range(len(topic)):
  a=topic[ii]
  g.add_vertices(1)
  g.vs[count]["label"]= a
  #g.vs[count]["type"] = "noun"
  g.vs[count]["type"] = "verb"
  count=count+1

for ii in range(len(topic)):
  word_list_temp=[]
  base_node=ii
  a=topic[ii]
  # g.vs[count]["label"]= a
  # g.vs[count]["type"] = "verb"

  for i in range(len(sent)):
    index=[]
    sen=sent[i]
    if a in sen:
      sen_t=sen_tex[i]
      sen_tg=sen_tag[i]
      # if 'noun' in sen_tg and 'verb' in sen_tg:
      #   index_noun = [i for i, x in enumerate(sen_tg) if x == 'noun']
      #   index_verb = [i for i, x in enumerate(sen_tg) if x == 'verb']
      #   index=index_noun+index_verb
      # else:
      #   if 'noun' in sen_tg:
      #     index = [i for i, x in enumerate(sen_tg) if x == 'noun']
      #   else:
      #     if 'verb' in sen_tg:
      #       index = [i for i, x in enumerate(sen_tg) if x == 'verb']

      if 'noun' in sen_tg:
        index = [i for i, x in enumerate(sen_tg) if x == 'noun']

        if len(index)>0:
          verb_l=[sen_t[i] for i in index]
          if type(verb_l) is list:
            for u in range(len(verb_l)):
              if verb_l[u] not in word_list:
                word_list.append(verb_l[u])
                # print(verb_l[u])
                g.add_vertices(1)
                word_label.append(g.vcount()-1)
                g.vs[count]["label"]=verb_l[u]
                g.vs[count]["type"] = "verb"
                g.add_edges([(base_node, count)])
                count=count+1
              else:
                ind=word_list.index(verb_l[u])
                #print(ind)
                g.add_edges([(base_node, word_label[ind])])

          else:
              if verb_l not in word_list:
                word_list.append(verb_l)
                # print(verb_l)
                g.add_vertices(1)
                word_label.append(g.vcount()-1)
                g.vs[count]["label"]=verb_l
                g.vs[count]["type"] = "verb"
                g.add_edges([(base_node, count)])
                count=count+1
              else:
                ind=word_list.index(verb_l)
                #print(ind)
                g.add_edges([(base_node, word_label[ind])])


                # if verb_l not in word_list_temp:
                #   word_list_temp.append(verb_l)
                #   ind=word_list.index(verb_l)
                #   if g.are_connected(base_node, word_label[ind])== False:
                #     g.add_edges([(base_node, word_label[ind])])

#######################Color for Verb Words ###################
############# DO not run otherwise-- will give error ##############
g.vs['label_color']='#BBB'
g.vs.find(label="SideEffects")['label_color']='#0a780c'
g.vs.find(label="confident")['label_color']='#0a780c'
g.vs.find(label="safe")['label_color']='#0a780c'
g.vs.find(label="worried")['label_color']='#fc3503'
g.vs.find(label="affect")['label_color']='#fc3503'
g.vs.find(label="witness")['label_color']='#fc3503'
g.vs.find(label="experiencing")['label_color']='#fc3503'
g.vs.find(label="effective")['label_color']='#0a780c'
g.vs.find(label="seem")['label_color']='#0a780c'
g.vs.find(label="take")['label_color']='#45063a'
g.vs.find(label="trust")['label_color']='#45063a'
g.vs.find(label="encourage")['label_color']='#45063a'

##### merging only for noun

g.vs.find(label="family")['label']='FamilyMembers'
g.vs.find(label="herd")['label']='HerdImmunity'

#######################Color for Noun###################
############# DO not run otherwise-- will give error ##############




g.vs['label_color']='#BBB'
g.vs.find(label="take")['label_color']='#45063a'
g.vs.find(label="relatives")['label_color']='#45063a'
g.vs.find(label="feedbacks")['label_color']='#45063a'
g.vs.find(label="Parents")['label_color']='#45063a'
g.vs.find(label="neighbors")['label_color']='#45063a'
g.vs.find(label="vaccinated")['label_color']='#45063a'
g.vs.find(label="people")['label_color']='#45063a'
g.vs.find(label="confidence")['label_color']='#45063a'
g.vs.find(label="peer")['label_color']='#45063a'
g.vs.find(label="Price")['label_color']='#45063a'

g.vs.find(label="effective")['label_color']='#0a780c'
g.vs.find(label="government")['label_color']='#0a780c'
g.vs.find(label="safe")['label_color']='#0a780c'
g.vs.find(label="benefit")['label_color']='#0a780c'

g.vs.find(label="SideEffects")['label_color']='#fcba03'


g.vs.find(label="harm")['label_color']='#fc3503'
g.vs.find(label="health")['label_color']='#fc3503'
g.vs.find(label="risk")['label_color']='#fc3503'
# g.vs.find(label="slot")['label_color']='#b5960b'
# g.vs.find(label="registration")['label_color']='#b5960b'
# g.vs.find(label="facility")['label_color']='#b5960b'

# g.vs.find(label="effective")['label_color']='#0a780c'
# g.vs.find(label="industries")['label_color']='#0a780c'
# g.vs.find(label="hotels")['label_color']='#0a780c'
# g.vs.find(label="HerdImmunity")['label_color']='#0a780c'
# g.vs.find(label="market")['label_color']='#0a780c'
# g.vs.find(label="restaurants")['label_color']='#0a780c'
# g.vs.find(label="tours")['label_color']='#0a780c'
# g.vs.find(label="travel")['label_color']='#0a780c'

deg=g.vs[0].degree()
layout = g.layout("kk")
#layout = g.layout_auto()
deg = g.degree()
#plot(g,"my_graph.pdf",layout=layout,vertex_label_size=9,vertex_size=5, vertex_font=2, vertex_color='#FF0')
plot(g,"TransitionFromHesitant_noun.pdf",layout=layout,keep_aspect_ratio=True, vertex_label_size=np.power(np.array(deg),0.2)*7,vertex_size=np.array(deg)*0.01, vertex_font=2, vertex_color='#DDD',edge_color='#EEE',vertex_frame_color='#FFF')
#plot(g,"No-->hesitant.pdf",layout=layout, vertex_font=2, vertex_color='#DDD',edge_color='#EEE',vertex_frame_color='#FFF')


files.download("TransitionFromHesitant_noun.pdf")

write(g,'No-hesitant_verb.gml',format='gml')