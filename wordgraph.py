# -*- coding: utf-8 -*-
"""WordGraph.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_eWMcbc4qO1Bo0ObwuzSZH5so1bZbl5k
"""

from google.colab import drive
drive.mount("/content/gdrive")

import pandas as pd
import numpy as np
import matplotlib
import matplotlib.pyplot as plt
import nltk
from process_utils import merge_words, merge_stop, extract_topics
from gensim.utils import simple_preprocess
from gensim.parsing.preprocessing import remove_stopwords, STOPWORDS

df=pd.read_excel('/content/gdrive/My Drive/DLDatasets/Shagata_paper2/merged data_1220resposes.xlsx')
df=df['Reason_hesitant']
df=merge_words(df)
df_raw=df

str_tot=''
for i in range(len(df)):
    str_in=df.iat[i]
    str_tot=str_tot+str_in+'. '

text1=str_tot
doc_complete=[text1]

from nltk.corpus import stopwords
nltk.download('wordnet')
nltk.download('stopwords')
nltk.download('omw-1.4')
from nltk.stem.wordnet import WordNetLemmatizer
import string
stop=merge_stop()
exclude = set(string.punctuation)
lemma = WordNetLemmatizer()
def clean(doc):
    stop_free = " ".join([i for i in doc.lower().split() if i not in stop])
    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)
    normalized = "".join(WordNetLemmatizer().lemmatize(word) for word in punc_free)
    return normalized

doc_clean = [clean(doc).split() for doc in doc_complete]

df=pd.Series(doc_clean[0])

from collections import Counter
results = Counter()
df.str.lower().str.split().apply(results.update)

total_term=len(results)
results_top10=results.most_common(10)
print(results_top10)

import matplotlib.pyplot as plt
from google.colab import files

matplotlib.rcParams['lines.linewidth'] = 4
plt.figure(figsize=(10, 7))
plt.title("Willing-->Hesitant",fontsize=18)
plt.xlabel('Terms',fontsize=16)
plt.ylabel('Normalized Frequency',fontsize=18)
plt.xticks(rotation='vertical',fontsize=12)
plt.yticks(fontsize=14)

names = list(dict(results_top10).keys())
values = list(dict(results_top10).values())

plt.bar(range(len(results_top10)), np.array(values)/total_term, tick_label=names,color=(0.2, 0.4, 0.6, 0.6))

plt.savefig("Why_H.png", bbox_inches='tight')
files.download("Why_H.png")
plt.show()
################## Delete 'also' 'like' 'getting' 'vaccinated' manually from the saved fig

!pip install spacy &> /dev/null
!python3 -m spacy download en &> /dev/null

A=df_raw
#stop=stop.union(set(['covid','virus','vaccine','Vaccine','Vaccines','vaccination', 'think','vaccines','taking','yes']))
stop=merge_stop()
A = A.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))
def format_sentence(sentence) :
    sen=[]
    sentenceSplit = filter(None, sentence.split("."))
    for s in sentenceSplit :
       s=s.replace(",","")
       sen.append(s+ ".")
    return sen


import spacy
nlp = spacy.load("en_core_web_sm")

sen_tex=[]
sen_tag=[]
sent=[]
for j in range(len(A)):
  ab=A.iat[j]
  sen= format_sentence(ab)
  for i in range(len(sen)):
    y=sen[i]
    xx=[]
    tt=[]
    doc = nlp(y)
    for token in doc:
      x=str(f'{spacy.explain(token.pos_)}')
      t=str(f'{token.text}')
      xx.append(x)
      tt.append(t)
    sent.append(y)
    sen_tex.append(tt)
    sen_tag.append(xx)

  sent.append('Break')
  sen_tex.append('Break')
  sen_tag.append('Break')

!pip install python-igraph==0.8.3 &> /dev/null
!apt install libcairo2-dev pkg-config python3-dev &> /dev/null
!pip install python-igraph leidenalg pycairo &> /dev/null
from igraph import *

g = Graph()
count=0
base_node=0
topic=extract_topics(results_top10,5)
#topic1=['method','potential']

word_list=topic
word_label=[0,1,2,3,4]
for ii in range(len(topic)):
  a=topic[ii]
  g.add_vertices(1)
  g.vs[count]["label"]= a
  #g.vs[count]["type"] = "noun"
  g.vs[count]["type"] = "verb"
  count=count+1

for ii in range(len(topic)):
  word_list_temp=[]
  base_node=ii
  a=topic[ii]
  # g.vs[count]["label"]= a
  # g.vs[count]["type"] = "verb"

  for i in range(len(sent)):
    index=[]
    sen=sent[i]
    if a in sen:
      sen_t=sen_tex[i]
      sen_tg=sen_tag[i]
      # if 'noun' in sen_tg and 'verb' in sen_tg:
      #   index_noun = [i for i, x in enumerate(sen_tg) if x == 'noun']
      #   index_verb = [i for i, x in enumerate(sen_tg) if x == 'verb']
      #   index=index_noun+index_verb
      # else:
      #   if 'noun' in sen_tg:
      #     index = [i for i, x in enumerate(sen_tg) if x == 'noun']
      #   else:
      #     if 'verb' in sen_tg:
      #       index = [i for i, x in enumerate(sen_tg) if x == 'verb']

      if 'noun' in sen_tg:
        index = [i for i, x in enumerate(sen_tg) if x == 'verb']

        if len(index)>0:
          verb_l=[sen_t[i] for i in index]
          if type(verb_l) is list:
            for u in range(len(verb_l)):
              if verb_l[u] not in word_list:
                word_list.append(verb_l[u])
                # print(verb_l[u])
                g.add_vertices(1)
                word_label.append(g.vcount()-1)
                g.vs[count]["label"]=verb_l[u]
                g.vs[count]["type"] = "verb"
                g.add_edges([(base_node, count)])
                count=count+1
              else:
                ind=word_list.index(verb_l[u])
                #print(ind)
                g.add_edges([(base_node, word_label[ind])])

          else:
              if verb_l not in word_list:
                word_list.append(verb_l)
                # print(verb_l)
                g.add_vertices(1)
                word_label.append(g.vcount()-1)
                g.vs[count]["label"]=verb_l
                g.vs[count]["type"] = "verb"
                g.add_edges([(base_node, count)])
                count=count+1
              else:
                ind=word_list.index(verb_l)
                #print(ind)
                g.add_edges([(base_node, word_label[ind])])


                # if verb_l not in word_list_temp:
                #   word_list_temp.append(verb_l)
                #   ind=word_list.index(verb_l)
                #   if g.are_connected(base_node, word_label[ind])== False:
                #     g.add_edges([(base_node, word_label[ind])])

deg=g.vs[0].degree()
#layout = g.layout("kk")
layout = g.layout_auto()
deg = g.degree()
#plot(g,"/content/drive/My Drive/DLDatasets/my_graph.pdf",layout=layout,vertex_label_size=9,vertex_size=5, vertex_font=2, vertex_color='#FF0')
plot(g,"WhyHesitantNoun.pdf",layout=layout,keep_aspect_ratio=True, vertex_label_size=np.power(np.array(deg),0.3)*6,vertex_size=np.array(deg)*0.01, vertex_font=2, vertex_color='#F55',edge_color='#EEE',vertex_frame_color='#FFF',bbox=(600,600))
files.download("WhyHesitantNoun.pdf")